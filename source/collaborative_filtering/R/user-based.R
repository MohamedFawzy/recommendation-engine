# load library for recommendation
library(recommenderlab)
# load dataset for jester5k
data("Jester5k")
# sample rating data of the first six users on the first 10 jokes.
head(as(Jester5k, "matrix")[, 1:10])
# Building a base recommender model for benchmarking by splitting the data into 80% training data and 20% test data.
# Evaluating the recommender model using a k-fold cross-validation approach model
# Parameter tuning for the recommender model

# Preparing the training data and test data
set.seed(1)
which_train <- sample(x = c(TRUE, FALSE), size = nrow(Jester5k), replace =TRUE, prob = c(0.8, 0.2))
head(which_train)

rec_data_train <- Jester5k[which_train, ]
rec_data_test  <- Jester5k[!which_train, ]
dim(rec_data_train)
dim(rec_data_test)
# explore models avaliable and their parameters in recommenderlab package
recommender_model <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
recommender_model
# build user-based collobrative filtering
recc_model <- Recommender(data = rec_data_train, method= "UBCF")
recc_model
recc_model@model$data
# predictions on test set
n_recommender <- 10
recc_predicit <- predict(object = recc_model, newdata = rec_data_test, n = n_recommender)
recc_predicit
# define list of predicited recommendations :
rec_list <- sapply(recc_predicit@items, function(x){
  colnames(Jester5k)[x]
})

rec_list

# count how many recommendation generated by test users 
number_of_items = sort(unlist(lapply(rec_list, length)), decreasing = TRUE)
table(number_of_items)
# extract the number ratings given to each joke
table(rowCounts(Jester5k))
# remove users who have rated 80 or more jokes
model_data = Jester5k[rowCounts(Jester5k) < 80]
dim(model_data)
#boxplot(model_data)

boxplot(rowMeans(model_data [rowMeans(model_data)>=-5 & rowMeans(model_data)<= 7]))
# drop users who have low average rating and high average rating
model_data = model_data[rowMeans(model_data) >= -5 & rowMeans(model_data) <= 7]
dim(model_data)
# examine the rating distribution of the first 100 users in the data
image(model_data, main = "Rating distribution of 100 users")
# evaluate the model using cross validation and 5-fold cross validation
items_to_keep <- 30
rating_threshold <- 3
n_fold <- 5
eval_sets <- evaluationScheme(data = model_data, method = "cross-validation",train = percentage_training,
                              given = items_to_keep, goodRating= rating_threshold, k = n_fold)
eval_sets

# size of five sets formed by the corss-validation
size_sets <- sapply(eval_sets@runsTrain, length)
size_sets
# extract data set e.g train , known the test set with the items to build recommender , unkown the test set with the items to test the recommender
getData(eval_sets, "train")
